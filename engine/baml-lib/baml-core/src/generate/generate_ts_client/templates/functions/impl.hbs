
{{#if is_chat}}

const prompt_template = [
  {{#each prompt}}
  {
    role: "{{role}}",
    content: `{{{content}}}`
  }{{#unless @last}},{{/unless}}
  {{/each}}
];

{{else}}
const prompt_template = `\
{{{prompt}}}\
`;
{{/if}}

const deserializer = new Deserializer<{{function.return_type}}>(schema, {
  $ref: '#/definitions/{{function.name}}_output'
});
{{#each overrides as |o|}}
deserializer.overload("{{o.category}}", {
  {{#each o.aliased_keys as |a|}}
  {{{a.alias}}}: "{{a.key}}",
  {{/each}}
});
{{/each}}

const {{name}} = async (
{{#if function.params.positional}}
  {{function.params.name}}: {{function.params.type}}
{{else}}
  {{function.params.name}}: {
    {{#each function.params.values as |v|}}{{v.name}}: {{v.type}}{{#unless @last}}, {{/unless}}{{/each}}
  }
{{/if}}
): Promise<{{function.return_type}}> => {
  {{#if function.params.positional}}
  {{!-- const {{function.params.name}} = {{function.params.name}}; --}}
  {{else}}
  {{#each function.params.values as |v|}}
  const {{v.name}} = {{{v.expr}}};
  {{/each}}
  {{/if}}
  
  const result = await {{client}}.run_{{#if is_chat}}chat{{else}}prompt{{/if}}_template(
    prompt_template,
    [{{#each inputs as |input key|}}
      "{{key}}",
    {{/each}}],
    {
      {{#each inputs as |input key|}}
      "{{key}}": {{input}},
      {{/each}}
    }
  );

  return deserializer.coerce(result.generated);
};

const {{name}}_stream = (
{{#if function.params.positional}}
  {{function.params.name}}: {{function.params.type}}
{{else}}
  {{function.params.name}}: {
    {{#each function.params.values as |v|}}{{v.name}}: {{v.type}}{{#unless @last}}, {{/unless}}{{/each}}
  }
{{/if}}
): LLMResponseStream<{{function.return_type}}> => {
  {{#if function.params.positional}}
  {{!-- const {{function.params.name}} = {{function.params.name}}; --}}
  {{else}}
  {{#each function.params.values as |v|}}
  const {{v.name}} = {{{v.expr}}};
  {{/each}}
  {{/if}}
  
  const stream = {{client}}.run_{{#if is_chat}}chat{{else}}prompt{{/if}}_template_stream(
    prompt_template,
    [{{#each inputs as |input key|}}
      "{{key}}",
    {{/each}}],
    {
      {{#each inputs as |input key|}}
      "{{key}}": {{input}},
      {{/each}}
    }
  );

  return new LLMResponseStream<{{function.return_type}}>(
    stream,
    (partial: string) => {
      console.log(`>>> partial >>>\n${partial}'\n<<< partial <<<`)
      return null
    },
    (final: string) => deserializer.coerce(final),
  );
};

{{function.name}}.registerImpl('{{name}}', {{name}}, {{name}}_stream);
